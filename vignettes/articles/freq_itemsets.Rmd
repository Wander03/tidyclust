---
title: "Frequent Itemset Mining"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Frequent Itemset Mining}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  markdown: 
    wrap: 72
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Setup

```{r}
library(workflows)
library(parsnip)
```

Load libraries:

```{r setup}
library(tidyclust)
library(arules)
set.seed(838383)
```

Load and clean a dataset:

```{r}
data(Groceries)

# convert to data frame
groceries <- as.data.frame(as(Groceries, "matrix")) %>%
  dplyr::mutate(across(everything(), ~.*1))
```

## A Brief Introduction to Frequent Itemset Mining

*Frequent Itemset Mining* is a fundamental technique in data mining that
identifies sets of items that frequently appear together in
transactional datasets. These itemsets are often used to uncover
meaningful patterns, such as associations between items, which can then
be leveraged to generate *association rules*.

For example, in a supermarket transaction database, frequent itemset
mining can identify groups of products that are commonly purchased
together, such as `{milk, bread, eggs}`. These insights are valuable for
applications like recommendation systems, inventory management, and
targeted marketing.

The key to frequent itemset mining is determining the sets of items that
satisfy a user-defined threshold called the **minimum support**, where
support is defined as the proportion of transactions in which a
particular itemset appears.

### Methods of Frequent Itemset Mining

Efficiently discovering these frequent itemsets is a computational
challenge, and several algorithms have been developed to address this
challenge. The two implemented in `{tidyclust}` are the **Apriori**
algorithm and the **Eclat** algorithm.

#### Finding Frequent Itemsets with the Apriori Algorithm

The *Apriori* algorithm is one of the earliest and most widely known
methods for frequent itemset mining. It is based on the **Apriori
Principle** (also known as **Downward Closure Property**): any subset of
a frequent itemset must also be frequent.

#### Process of the Apriori Algorithm

1.  **Initialization**: Begin by identifying all individual items
    (1-itemsets) that satisfy the minimum support threshold. These are
    called *frequent 1-itemsets*.

2.  **Candidate Generation**: Use the frequent itemsets from the
    previous step to generate candidate itemsets of the next size (e.g.
    combine frequent 1-itemsets to create candidate 2-itemsets).

3.  **Prune Candidates**: Eliminate candidate itemsets that have subsets
    not found to be frequent.

4.  **Support Counting**: Scan the dataset to count the occurrences of
    each candidate itemset.

5.  **Iteration**: Repeat steps 2–4 for larger itemsets until no more
    frequent itemsets can be generated.

[![Apriori Princple
Example](images/clipboard-4074102307.png){width="650"}](https://chih-ling-hsu.github.io/2017/03/25/apriori)

The Apriori algorithm is computationally expensive due to repeated
database scans and the generation of numerous candidates. However, its
pruning strategy significantly reduces the search space compared to a
naïve approach.

[Source](https://dl.acm.org/doi/pdf/10.1145/170036.170072)

#### Finding Frequent Itemsets with the Eclat Algorithm

The *Eclat* (Equivalence Class Transformation) algorithm is an
alternative to Apriori that uses a depth-first search strategy and
vertical data representation. Instead of scanning the dataset
repeatedly, Eclat represents transactions as *tid-lists* (transaction ID
lists), which map each item or itemset to the IDs of transactions in
which it appears.

#### Process of the Eclat Algorithm

1.  **Vertical Data Representation**: Transform the dataset into a
    vertical format, where each item is associated with a list of
    transaction IDs.

2.  **Intersect Tid-lists**: Generate frequent itemsets by recursively
    intersecting the tid-lists of individual items to form larger
    itemsets. The intersection results in a new tid-list, representing
    the transactions containing the larger itemset.

3.  **Check Support**: The length of the resulting tid-list determines
    the support of the itemset. Remove itemsets not found to be
    frequent.

4.  **Recursive Search**: Continue the process for all itemsets until no
    further frequent itemsets can be found.

[![Bookstore
database](images/clipboard-2860408154.png){width="325"}](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=846291)

[![Computing support of itemsets via tid-list
intersections](images/clipboard-3198084587.png){width="650"}](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=846291)

Eclat is generally more efficient than Apriori for datasets with many
transactions but fewer unique items, as it avoids the need for multiple
scans of the dataset. However, its performance can degrade for datasets
with very large tid-lists.

[Source](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=846291)

## **`freq_itemsets` specification in {tidyclust}**

To specify a frequent itemsets mining model in `tidyclust`, simply
choose a value of `min_support` and (optionally) a mining method:

```{r}
fi_spec <- freq_itemsets(min_support = 0.05, 
                           mining_method = "apriori") %>%
  set_engine("arules") %>%
  set_mode("partition")

fi_spec
```

Currently, the only supported engine is `arules`. The default mining
method is apriori.

## **Fitting `freq_itemsets` models**

We fit the model to the data in the usual way:

```{r}
fi_fit <- fi_spec %>%
  fit(~ .,
    data = groceries
  )

fi_fit %>%
  summary()
```

We can also extract the standard `tidyclust` summary list:

```{r}
# fi_summary <- fi_fit %>%
#   extract_fit_summary()
# 
# fi_summary %>%
#   str()
```

Note that, although the frequent itemset algorithm is not focused on cluster's
like other unsupervised learning algorithms, we have created clusters based on
the itemsets. For each item, we find all itemsets that include that item:

- Itemsets with the largest size are selected as the "dominate" itemset for the item

- If there is a tie in size, the itemset with the highest support is selected

- If no itemsets include the item, it is assigned a special "outlier" cluster (Cluster_0_1, Cluster_0_2, etc.)

```{r}
fi_fit %>% 
  extract_cluster_assignment()
```

## Prediction

Since frequent itemset mining identifies patterns in co-occurring items rather 
than learning a predictive function, the notion of "prediction" is not as 
straightforward as in supervised learning. However, given a set of frequent 
itemsets from historical data, it is possible to estimate the likelihood that a 
missing item in new data is present based on observed co-occurring items.

The predict() function utilizes frequent itemsets and their support values to 
estimate probabilities for missing items in new transactions. For each row in 
new_data, the function identifies observed items and missing items. It then 
searches for frequent itemsets that contain both the missing item and at least 
one observed item. Using the support values of these itemsets, it estimates the 
probability that the missing item is present based on the confidence of 
association between observed and missing items.

The function fills in missing values with these probability estimates, 
effectively "predicting" the likelihood of item presence based on historical 
co-occurrence patterns.

We display the predicted values in the column `.pred_item`, and the observed 
values in the column `.obs_item`.

```{r}
new_data <- groceries[1:5,] %>%
  dplyr::mutate(`whole milk` = as.numeric(NA),
                `other vegetables` = as.numeric(1))

results <- fi_fit %>%
  predict(new_data)

results[[1,1]]
```

Additionally, we can extract the nested predicted output to be formatted in a 
single data frame, filling in the `NA` values with their predicted value using
`extract_predictions`.

```{r}
results %>%
  extract_predictions
```




```{r}
mvb_cvs <- rsample::vfold_cv(groceries, v = 10)

rf_grid <- dials::grid_regular(min_support(), 
                        levels = 10)

rf_mod_tune <- freq_itemsets(min_support = tune(), 
                           mining_method = "apriori") %>%
  set_engine("arules") %>%
  set_mode("partition")

rf_recipe <- recipes::recipe(~ ., data = groceries)

rf_wflow_tune <- workflow() %>%
  add_model(rf_mod_tune) %>% 
  add_recipe(rf_recipe)

rf_grid_search <-
  tune::tune_grid(
    rf_wflow_tune,
    resamples = mvb_cvs,
    grid = rf_grid
  )
```





---
title: "Frequent Itemset Mining"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Frequent Itemset Mining}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  markdown: 
    wrap: 72
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Setup

```{r}
library(workflows)
library(parsnip)
```

Load libraries:

```{r setup}
library(tidyclust)
library(arules)
set.seed(838383)
```

Load and clean a dataset:

```{r}
data(Groceries)

# convert to data frame and take subset
groceries <- as.data.frame(as(Groceries[1:100,], "matrix"))
```

## A Brief Introduction to Frequent Itemset Mining

*Frequent Itemset Mining* is a fundamental technique in data mining that
identifies sets of items that frequently appear together in
transactional datasets. These itemsets are often used to uncover
meaningful patterns, such as associations between items, which can then
be leveraged to generate *association rules*.

For example, in a supermarket transaction database, frequent itemset
mining can identify groups of products that are commonly purchased
together, such as `{milk, bread, eggs}`. These insights are valuable for
applications like recommendation systems, inventory management, and
targeted marketing.

The key to frequent itemset mining is determining the sets of items that
satisfy a user-defined threshold called the **minimum support**, where
support is defined as the proportion of transactions in which a
particular itemset appears.

### Methods of Frequent Itemset Mining

Efficiently discovering these frequent itemsets is a computational
challenge, and several algorithms have been developed to address this
challenge. The two implemented in `{tidyclust}` are the **Apriori**
algorithm and the **Eclat** algorithm.

#### Finding Frequent Itemsets with the Apriori Algorithm

The *Apriori* algorithm is one of the earliest and most widely known
methods for frequent itemset mining. It is based on the **Apriori
Principle** (also known as **Downward Closure Property**): any subset of
a frequent itemset must also be frequent.

#### Process of the Apriori Algorithm

1.  **Initialization**: Begin by identifying all individual items
    (1-itemsets) that satisfy the minimum support threshold. These are
    called *frequent 1-itemsets*.

2.  **Candidate Generation**: Use the frequent itemsets from the
    previous step to generate candidate itemsets of the next size (e.g.
    combine frequent 1-itemsets to create candidate 2-itemsets).

3.  **Prune Candidates**: Eliminate candidate itemsets that have subsets
    not found to be frequent.

4.  **Support Counting**: Scan the dataset to count the occurrences of
    each candidate itemset.

5.  **Iteration**: Repeat steps 2–4 for larger itemsets until no more
    frequent itemsets can be generated.

[![Apriori Princple
Example](images/clipboard-4074102307.png){width="650"}](https://chih-ling-hsu.github.io/2017/03/25/apriori)

The Apriori algorithm is computationally expensive due to repeated
database scans and the generation of numerous candidates. However, its
pruning strategy significantly reduces the search space compared to a
naïve approach.

[Source](https://dl.acm.org/doi/pdf/10.1145/170036.170072)

#### Finding Frequent Itemsets with the Eclat Algorithm

The *Eclat* (Equivalence Class Transformation) algorithm is an
alternative to Apriori that uses a depth-first search strategy and
vertical data representation. Instead of scanning the dataset
repeatedly, Eclat represents transactions as *tid-lists* (transaction ID
lists), which map each item or itemset to the IDs of transactions in
which it appears.

#### Process of the Eclat Algorithm

1.  **Vertical Data Representation**: Transform the dataset into a
    vertical format, where each item is associated with a list of
    transaction IDs.

2.  **Intersect Tid-lists**: Generate frequent itemsets by recursively
    intersecting the tid-lists of individual items to form larger
    itemsets. The intersection results in a new tid-list, representing
    the transactions containing the larger itemset.

3.  **Check Support**: The length of the resulting tid-list determines
    the support of the itemset. Remove itemsets not found to be
    frequent.

4.  **Recursive Search**: Continue the process for all itemsets until no
    further frequent itemsets can be found.

[![Bookstore
database](images/clipboard-2860408154.png){width="325"}](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=846291)

[![Computing support of itemsets via tid-list
intersections](images/clipboard-3198084587.png){width="650"}](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=846291)

Eclat is generally more efficient than Apriori for datasets with many
transactions but fewer unique items, as it avoids the need for multiple
scans of the dataset. However, its performance can degrade for datasets
with very large tid-lists.

[Source](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=846291)

## **`freq_itemsets` specification in {tidyclust}**

To specify a frequent itemsets mining model in `tidyclust`, simply
choose a value of `min_support` and (optionally) a mining method:

```{r}
fi_spec <- freq_itemsets(
  min_support = 0.2,
  mining_method = "apriori"
)

fi_spec
```

Currently, the only supported engine is `arules`. The default mining
method is apriori.

## **Fitting `freq_itemsets` models**

We fit the model to the data in the usual way:

```{r}
fi_fit <- fi_spec %>%
  fit(~ .,
    data = groceries
  )

fi_fit %>%
  summary()
```

We can also extract the standard `tidyclust` summary list:

```{r}
# fi_summary <- fi_fit %>%
#   extract_fit_summary()
# 
# fi_summary %>%
#   str()
```

```{r}
#| include: false
#| eval: false

d <- as.data.frame(as(Groceries, "matrix"))

fi_spec <- freq_itemsets(
  min_support = 0.2,
  mining_method = "eclat"
)

fi_fit <- fi_spec %>%
  fit(~ .,
    data = d
  )

inspect(fi_fit$fit)

fi_fit$spec$method$fit$args$

# fi_fit %>%
#   summary()

#####################################################################
test <- arules::eclat(
  data = d, 
  parameter = list(support = 0.05))

arules::inspect(test)

items <- colnames(d)
itemsets <- arules::inspect(test)
itemset_list <- lapply(strsplit(gsub("[{}]", "", itemsets$items), ","), stringr::str_trim)

support <- itemsets$support
clusters <- numeric(length(items))

sapply(1:length(items), function(i) {
  current_item <- items[i]
  
  # Find relevant itemsets that contain the current itemset
  relevant_itemsets <- which(sapply(itemset_list, function(x) current_item %in% x))

  
  if (length(relevant_itemsets) == 0) {
    return(0)  # No frequent itemsets, assign to own cluster
  }

  # Find all items in relevant itemsets
  all_items <- unique(unlist(itemset_list[relevant_itemsets]))
  all_items_num <- match(all_items, items)
  
  # Highest support with largest itemset tiebreaker
  best_itemset <- relevant_itemsets[which.max(support[relevant_itemsets])]
  
  if (clusters[i] == 0 || support[best_itemset] > support[clusters[i]]) {
    for (x in all_items_num) {
      clusters[x] <<- best_itemset
    }
  }
})

n_clusters <- length(unique(clusters))
prefix = "Cluster_"

# Vector to store the resulting cluster names
res <- character(length(clusters))

# For items with cluster value 0, assign to "Cluster_0"
res[clusters == 0] <- "Cluster_0"
zero_count <- 0
res <- sapply(res, function(x) {
  if (x == "Cluster_0") {
    zero_count <<- zero_count + 1
    paste0("Cluster_0_", zero_count)
  } else {
    x
  }
})

# For non-zero clusters, assign sequential cluster numbers starting from "Cluster_1"
non_zero_clusters <- clusters[clusters != 0]
unique_non_zero_clusters <- unique(non_zero_clusters)

# Map each unique non-zero cluster to a new cluster starting from Cluster_1
cluster_map <- setNames(paste0(prefix, seq_along(unique_non_zero_clusters)), unique_non_zero_clusters)

# Assign the corresponding cluster names to the non-zero clusters
res[clusters != 0] <- cluster_map[as.character(non_zero_clusters)]

tibble::tibble(.cluster = factor(res)) %>%
  group_by(.cluster) %>%
  summarise(n())

#####################################################################
```

## Prediction

***[WIP]***
